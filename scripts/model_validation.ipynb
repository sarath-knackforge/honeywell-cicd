{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f077732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enterprise MLOps: Model Validation & Deployment Gate\n",
    "\n",
    "Responsibilities:\n",
    "- Resolve currently deployed model (alias-first)\n",
    "- Load candidate run metrics & params from MLflow\n",
    "- Enforce metric + parameter thresholds\n",
    "- Emit CI/CD-friendly outputs\n",
    "- Block deployment if validation fails\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Logging\n",
    "# ------------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"model-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ff090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Databricks Context\n",
    "# ------------------------------------------------------------------------------\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Threshold Configuration (environment-aware if needed later)\n",
    "# ------------------------------------------------------------------------------\n",
    "METRIC_THRESHOLDS = {\n",
    "    \"accuracy\": 0.85,\n",
    "    \"f1_score\": 0.80,\n",
    "    \"precision\": 0.75,\n",
    "    \"recall\": 0.75,\n",
    "    # lower is better\n",
    "    \"log_loss\": 0.70,\n",
    "}\n",
    "\n",
    "REQUIRED_PARAMS = {\n",
    "    # Example: enforce reproducibility / governance\n",
    "    \"learning_rate\": (0.001, 0.5),\n",
    "    \"num_leaves\": (8, 512),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------------------------\n",
    "def emit_ci_output(key: str, value: str) -> None:\n",
    "    print(f\"::mlflow-run-output::{key}={value}\")\n",
    "\n",
    "def set_task_value(key: str, value: str) -> None:\n",
    "    try:\n",
    "        dbutils.jobs.taskValues.set(key=key, value=value)\n",
    "    except Exception as exc:\n",
    "        logger.warning(\"Failed to set task value %s: %s\", key, exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resolve_deployed_model(\n",
    "    client: MlflowClient,\n",
    "    model_name: str,\n",
    "    alias: str = \"champion\"\n",
    "):\n",
    "    try:\n",
    "        mv = client.get_model_version_by_alias(model_name, alias)\n",
    "        logger.info(\n",
    "            \"Resolved deployed model via alias '%s': version=%s run_id=%s\",\n",
    "            alias, mv.version, mv.run_id\n",
    "        )\n",
    "        return str(mv.version), mv.run_id\n",
    "    except Exception:\n",
    "        logger.info(\"Alias '%s' not found for model %s.\", alias, model_name)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e58fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_metric_thresholds(\n",
    "    metrics: Dict[str, float],\n",
    "    thresholds: Dict[str, float]\n",
    ") -> list[str]:\n",
    "    failures = []\n",
    "\n",
    "    for name, threshold in thresholds.items():\n",
    "        if name not in metrics:\n",
    "            failures.append(f\"{name}: missing\")\n",
    "            continue\n",
    "\n",
    "        value = float(metrics[name])\n",
    "\n",
    "        if name == \"log_loss\":\n",
    "            if value > threshold:\n",
    "                failures.append(f\"{name}: {value:.4f} > {threshold}\")\n",
    "        else:\n",
    "            if value < threshold:\n",
    "                failures.append(f\"{name}: {value:.4f} < {threshold}\")\n",
    "\n",
    "    return failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd709290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_param_constraints(\n",
    "    params: Dict[str, str],\n",
    "    constraints: Dict[str, tuple]\n",
    ") -> list[str]:\n",
    "    failures = []\n",
    "\n",
    "    for name, (low, high) in constraints.items():\n",
    "        if name not in params:\n",
    "            failures.append(f\"{name}: missing\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            value = float(params[name])\n",
    "        except Exception:\n",
    "            failures.append(f\"{name}: not numeric ({params[name]})\")\n",
    "            continue\n",
    "\n",
    "        if not (low <= value <= high):\n",
    "            failures.append(f\"{name}: {value} not in [{low}, {high}]\")\n",
    "\n",
    "    return failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Core Logic\n",
    "# ------------------------------------------------------------------------------\n",
    "def validate_candidate_model(\n",
    "    env: str,\n",
    "    model_name: str,\n",
    "    candidate_run_id: str\n",
    ") -> None:\n",
    "    logger.info(\n",
    "        \"Starting model validation | env=%s model_name=%s candidate_run_id=%s\",\n",
    "        env, model_name, candidate_run_id\n",
    "    )\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Fetch candidate run\n",
    "    # --------------------------------------------------------------------------\n",
    "    try:\n",
    "        run = client.get_run(candidate_run_id)\n",
    "    except Exception as exc:\n",
    "        logger.error(\"Candidate run not found: %s\", exc)\n",
    "        raise RuntimeError(\"Invalid candidate_run_id\") from exc\n",
    "\n",
    "    candidate_metrics = run.data.metrics or {}\n",
    "    candidate_params = run.data.params or {}\n",
    "\n",
    "    logger.info(\"Candidate metrics: %s\", candidate_metrics)\n",
    "    logger.info(\"Candidate params: %s\", candidate_params)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Resolve currently deployed model (Champion)\n",
    "    # --------------------------------------------------------------------------\n",
    "    deployed_version, deployed_run_id = resolve_deployed_model(\n",
    "        client, model_name, alias=\"champion\"\n",
    "    )\n",
    "\n",
    "    if deployed_version:\n",
    "        set_task_value(\"deployed_model_version\", deployed_version)\n",
    "        set_task_value(\"deployed_run_id\", deployed_run_id)\n",
    "        emit_ci_output(\"deployed_model_version\", deployed_version)\n",
    "    else:\n",
    "        logger.info(\"No deployed Champion found. Treating as first deployment.\")\n",
    "        set_task_value(\"deployed_model_version\", \"0\")\n",
    "        emit_ci_output(\"deployed_model_version\", \"0\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Metric Threshold Validation\n",
    "    # --------------------------------------------------------------------------\n",
    "    metric_failures = evaluate_metric_thresholds(\n",
    "        candidate_metrics, METRIC_THRESHOLDS\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Parameter Constraint Validation\n",
    "    # --------------------------------------------------------------------------\n",
    "    param_failures = evaluate_param_constraints(\n",
    "        candidate_params, REQUIRED_PARAMS\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Final Decision\n",
    "    # --------------------------------------------------------------------------\n",
    "    failures = metric_failures + param_failures\n",
    "\n",
    "    if failures:\n",
    "        logger.error(\"❌ Model validation FAILED\")\n",
    "        for f in failures:\n",
    "            logger.error(\"   - %s\", f)\n",
    "\n",
    "        emit_ci_output(\"model_validation\", \"FAILED\")\n",
    "        set_task_value(\"model_validation\", \"FAILED\")\n",
    "\n",
    "        raise RuntimeError(\n",
    "            \"Candidate model failed validation thresholds. \"\n",
    "            \"Blocking deployment.\"\n",
    "        )\n",
    "\n",
    "    logger.info(\"✅ Model validation PASSED\")\n",
    "    emit_ci_output(\"model_validation\", \"PASSED\")\n",
    "    set_task_value(\"model_validation\", \"PASSED\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Emit promotion metadata\n",
    "    # --------------------------------------------------------------------------\n",
    "    emit_ci_output(\"candidate_run_id\", candidate_run_id)\n",
    "    set_task_value(\"candidate_run_id\", candidate_run_id)\n",
    "\n",
    "    logger.info(\"Model validation completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe712a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CLI\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Enterprise MLOps Model Validation\")\n",
    "    parser.add_argument(\"--env\", required=True, help=\"Environment name\")\n",
    "    parser.add_argument(\"--model_name\", required=True, help=\"Registered model name\")\n",
    "    parser.add_argument(\n",
    "        \"--candidate_run_id\",\n",
    "        required=True,\n",
    "        help=\"MLflow run_id of trained candidate model\"\n",
    "    )\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        logger.info(\"Ignoring unknown args: %s\", unknown)\n",
    "\n",
    "    try:\n",
    "        validate_candidate_model(\n",
    "            env=args.env,\n",
    "            model_name=args.model_name,\n",
    "            candidate_run_id=args.candidate_run_id\n",
    "        )\n",
    "    except Exception:\n",
    "        logger.exception(\"Model validation failed\")\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
