{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff757ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ..\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b578b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# NOW safe to import sklearn / pandas / mlflow.pyfunc\n",
    "# ------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import lightgbm\n",
    "import mlflow.pyfunc\n",
    "import sys\n",
    "import subprocess\n",
    "import site\n",
    "import mlflow\n",
    "from loguru import logger\n",
    "\n",
    "logger.info(\"Runtime versions:\")\n",
    "logger.info(\"  mlflow=%s\", mlflow.__version__)\n",
    "logger.info(\"  sklearn=%s\", sklearn.__version__)\n",
    "logger.info(\"  pandas=%s\", pd.__version__)\n",
    "logger.info(\"  lightgbm=%s\", lightgbm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enterprise MLOps: Model Validation & Deployment Gate (Databricks + MLflow)\n",
    "\n",
    "Responsibilities:\n",
    "- Consume exact registered model artifact via model URI (chain of custody)\n",
    "- Run deployability checks (artifact, signature, inference)\n",
    "- Load candidate run metrics & params\n",
    "- Enforce metric + parameter thresholds\n",
    "- Resolve currently deployed model (alias-first)\n",
    "- Attach Challenger alias on pass\n",
    "- Emit CI/CD-friendly outputs\n",
    "- Block deployment if validation fails\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, Optional, Tuple\n",
    "import subprocess\n",
    "import sys\n",
    "from honeywell.config import ProjectConfig\n",
    "from honeywell.run_inference_chk import run_real_data_inference_sanity_check\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlflow.models import get_model_info\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "import builtins\n",
    "list = builtins.list \n",
    "\n",
    "config = ProjectConfig.from_yaml(config_path=\"../project_config_honeywell.yml\", env=\"dev\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Logging (centralized, CI-friendly)\n",
    "# ------------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"model-validation\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Databricks Context\n",
    "# ------------------------------------------------------------------------------\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Threshold Configuration (env-aware if needed later)\n",
    "# ------------------------------------------------------------------------------\n",
    "METRIC_THRESHOLDS = {\n",
    "    \"accuracy\": 0.85,\n",
    "    \"f1_score\": 0.80,\n",
    "    \"precision\": 0.75,\n",
    "    \"recall\": 0.75,\n",
    "    \"roc_auc\": 0.85,\n",
    "    # lower is better\n",
    "    \"log_loss\": 0.5,\n",
    "}\n",
    "\n",
    "REQUIRED_PARAMS = {\n",
    "    # Basic governance for LightGBM training params (Phase-2+ optional)\n",
    "    \"learning_rate\": (0.0005, 0.1),\n",
    "    \"n_estimators\": (100, 3000),\n",
    "    \"max_depth\": (2, 12),\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Helpers: CI/CD outputs + Databricks task values\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def emit_ci_output(key: str, value: str) -> None:\n",
    "    # Generic CI-friendly output (harmless in Databricks)\n",
    "    print(f\"::mlflow-run-output::{key}={value}\")\n",
    "\n",
    "\n",
    "def set_task_value(key: str, value: str) -> None:\n",
    "    try:\n",
    "        dbutils.jobs.taskValues.set(key=key, value=value)\n",
    "    except Exception as exc:\n",
    "        logger.warning(\"Failed to set task value %s: %s\", key, exc)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Helpers: model registry resolution\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def resolve_deployed_model(\n",
    "    client: MlflowClient,\n",
    "    model_name: str,\n",
    "    alias: str = \"champion\",\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    try:\n",
    "        mv = client.get_model_version_by_alias(model_name, alias)\n",
    "        logger.info(\n",
    "            \"Resolved deployed model via alias '%s': version=%s run_id=%s\",\n",
    "            alias,\n",
    "            mv.version,\n",
    "            mv.run_id,\n",
    "        )\n",
    "        return str(mv.version), mv.run_id\n",
    "    except Exception:\n",
    "        logger.info(\"Alias '%s' not found for model %s.\", alias, model_name)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Helpers: metric + parameter validation\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def evaluate_metric_thresholds(\n",
    "    metrics: Dict[str, float],\n",
    "    thresholds: Dict[str, float],\n",
    ") -> list[str]:\n",
    "    failures: list[str] = []\n",
    "\n",
    "    for name, threshold in thresholds.items():\n",
    "        if name not in metrics:\n",
    "            failures.append(f\"{name}: missing\")\n",
    "            continue\n",
    "\n",
    "        value = float(metrics[name])\n",
    "\n",
    "        # Directionality\n",
    "        if name == \"log_loss\":\n",
    "            if value > threshold:\n",
    "                failures.append(f\"{name}: {value:.4f} > {threshold}\")\n",
    "        else:\n",
    "            if value < threshold:\n",
    "                failures.append(f\"{name}: {value:.4f} < {threshold}\")\n",
    "\n",
    "    return failures\n",
    "\n",
    "\n",
    "def evaluate_param_constraints(\n",
    "    params: Dict[str, str],\n",
    "    constraints: Dict[str, tuple],\n",
    ") -> list[str]:\n",
    "    failures: list[str] = [] \n",
    "\n",
    "    for name, (low, high) in constraints.items():\n",
    "        if name not in params:\n",
    "            failures.append(f\"{name}: missing\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            value = float(params[name])\n",
    "        except Exception:\n",
    "            failures.append(f\"{name}: not numeric ({params[name]})\")\n",
    "            continue\n",
    "\n",
    "        if not (low <= value <= high):\n",
    "            failures.append(f\"{name}: {value} not in [{low}, {high}]\")\n",
    "\n",
    "    return failures\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase-1 Deployability Checks (Databricks MLOps Book)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def run_deployability_checks(model_uri: str) -> None:\n",
    "    logger.info(\"Running deployability checks on model URI: %s\", model_uri)\n",
    "\n",
    "    # 1) Load model\n",
    "    try:\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "    except Exception as exc:\n",
    "        logger.error(\"‚ùå Failed to load model from URI: %s\", model_uri)\n",
    "        raise RuntimeError(\"Model artifact cannot be loaded\") from exc\n",
    "\n",
    "    logger.info(\"‚úÖ Model loads successfully from URI\")\n",
    "\n",
    "    # 2) Assert artifacts exist\n",
    "    local_path = mlflow.artifacts.download_artifacts(model_uri)\n",
    "\n",
    "    mlmodel_path = os.path.join(local_path, \"MLmodel\")\n",
    "    if not os.path.exists(mlmodel_path):\n",
    "        raise RuntimeError(\"MLmodel file missing in artifact directory\")\n",
    "\n",
    "    binary_found = any(\n",
    "        f.endswith((\".pkl\", \".pt\", \".onnx\", \".joblib\", \".bin\"))\n",
    "        for f in os.listdir(local_path)\n",
    "    )\n",
    "\n",
    "    if not binary_found:\n",
    "        raise RuntimeError(\"No model binary found in artifact directory\")\n",
    "\n",
    "    logger.info(\"‚úÖ Artifact existence checks passed\")\n",
    "\n",
    "    # 3) Assert signature exists\n",
    "    info = get_model_info(model_uri)\n",
    "\n",
    "    if info.signature is None:\n",
    "        raise RuntimeError(\"Model signature is missing\")\n",
    "\n",
    "    logger.info(\"‚úÖ Model signature exists\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Get real data\n",
    "        test_data = run_real_data_inference_sanity_check(\n",
    "            model=model,\n",
    "            model_uri=model_uri,\n",
    "            spark=spark,\n",
    "            config=config,\n",
    "            n_rows=1,\n",
    "        )\n",
    "    \n",
    "    #     # --- STEP 2: CONVERT REAL TEST DATA TO WORKING FORMAT ---\n",
    "    #     # Take the first row of your test data and convert to a list of one dictionary\n",
    "        real_row_dict = test_data.head(1).to_dict(orient='records')\n",
    "    #     # Re-create the DataFrame from that dictionary (matches your 'working format')\n",
    "        sample_df = pd.DataFrame(real_row_dict)\n",
    "\n",
    "    #     # --- STEP 3: STRICT TYPE CASTING (Mandatory for 2026) ---\n",
    "    #     # This ensures no 'category' types remain, preventing the <U0 error\n",
    "        sample_df = sample_df.astype({\n",
    "            \"SOC\": \"float64\",\n",
    "            \"Voltage\": \"float64\",\n",
    "            \"Current\": \"float64\",\n",
    "            \"Battery_Temp\": \"float64\",\n",
    "            \"Ambient_Temp\": \"float64\",\n",
    "            \"Charging_Duration\": \"float64\",\n",
    "            \"Degradation_Rate\": \"float64\",\n",
    "            \"Efficiency\": \"float64\",\n",
    "            \"Charging_Cycles\": \"int64\",\n",
    "            \"Battery_Type\": \"str\",\n",
    "            \"EV_Model\": \"str\",\n",
    "            \"Charging_Mode\": \"str\"\n",
    "        })\n",
    "\n",
    "    #     # --- STEP 4: PREDICT ---\n",
    "    #     logger.info(\"Executing prediction with cleaned real-data row...\")\n",
    "        preds = model.predict(sample_df)\n",
    "        logger.info(\"‚úÖ Prediction successful: %s\", preds )\n",
    "    except Exception as preds:\n",
    "        logger.error(\"‚ùå Prediction execution failed: %s\", preds)\n",
    "        raise preds\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Core Validation Logic\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def validate_candidate_model(\n",
    "    env: str,\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    candidate_run_id: str,\n",
    ") -> None:\n",
    "    logger.info(\n",
    "        \"Starting model validation | env=%s model_name=%s model_uri=%s run_id=%s\",\n",
    "        env,\n",
    "        model_name,\n",
    "        model_uri,\n",
    "        candidate_run_id,\n",
    "    )\n",
    "\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Phase-1: Deployability checks on registered artifact\n",
    "    # --------------------------------------------------------------------------\n",
    "    run_deployability_checks(model_uri)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Fetch candidate run\n",
    "    # --------------------------------------------------------------------------\n",
    "    try:\n",
    "        run = client.get_run(candidate_run_id)\n",
    "    except Exception as exc:\n",
    "        logger.error(\"Candidate run not found: %s\", exc)\n",
    "        raise RuntimeError(\"Invalid candidate_run_id\") from exc\n",
    "\n",
    "    candidate_metrics = run.data.metrics or {}\n",
    "    candidate_params = run.data.params or {}\n",
    "\n",
    "    logger.info(\"Candidate metrics: %s\", candidate_metrics)\n",
    "    logger.info(\"Candidate params: %s\", candidate_params)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Resolve currently deployed model (Champion)\n",
    "    # --------------------------------------------------------------------------\n",
    "    deployed_version, deployed_run_id = resolve_deployed_model(\n",
    "        client, model_name, alias=\"champion\"\n",
    "    )\n",
    "\n",
    "    if deployed_version:\n",
    "        set_task_value(\"deployed_model_version\", deployed_version)\n",
    "        set_task_value(\"deployed_run_id\", deployed_run_id or \"\")\n",
    "        emit_ci_output(\"deployed_model_version\", deployed_version)\n",
    "    else:\n",
    "        logger.info(\"No deployed Champion found. Treating as first deployment.\")\n",
    "        set_task_value(\"deployed_model_version\", \"0\")\n",
    "        emit_ci_output(\"deployed_model_version\", \"0\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Metric Threshold Validation\n",
    "    # --------------------------------------------------------------------------\n",
    "    metric_failures = evaluate_metric_thresholds(\n",
    "        candidate_metrics, METRIC_THRESHOLDS\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Parameter Constraint Validation\n",
    "    # --------------------------------------------------------------------------\n",
    "    param_failures = evaluate_param_constraints(\n",
    "        candidate_params, REQUIRED_PARAMS\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # LightGBM-specific governance: num_leaves <= 2^max_depth\n",
    "    # ----------------------------------------------------------------------\n",
    "    try:\n",
    "        max_depth = int(candidate_params.get(\"max_depth\", -1))\n",
    "        num_leaves = int(candidate_params.get(\"num_leaves\", -1))\n",
    "\n",
    "        if max_depth > 0 and num_leaves > 0:\n",
    "            max_allowed_leaves = 2 ** max_depth\n",
    "            if num_leaves > max_allowed_leaves:\n",
    "                param_failures.append(\n",
    "                    f\"num_leaves {num_leaves} > 2^max_depth ({max_allowed_leaves})\"\n",
    "                )\n",
    "    except Exception as exc:\n",
    "        param_failures.append(f\"num_leaves/max_depth validation error: {exc}\")\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Final Decision\n",
    "    # --------------------------------------------------------------------------\n",
    "    failures = metric_failures + param_failures\n",
    "\n",
    "    if failures:\n",
    "        logger.error(\"‚ùå Model validation FAILED\")\n",
    "        for f in failures:\n",
    "            logger.error(\"   - %s\", f)\n",
    "\n",
    "        emit_ci_output(\"model_validation\", \"FAILED\")\n",
    "        set_task_value(\"model_validation\", \"FAILED\")\n",
    "\n",
    "        # Mark model version as failed (optional governance)\n",
    "        try:\n",
    "            mv = client.get_model_version_by_alias(model_name, \"Challenger\")\n",
    "            client.set_model_version_tag(\n",
    "                name=model_name,\n",
    "                version=mv.version,\n",
    "                key=\"validation_status\",\n",
    "                value=\"failed\",\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        raise RuntimeError(\n",
    "            \"Candidate model failed validation thresholds. \"\n",
    "            \"Blocking deployment.\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # PASS: Attach Challenger alias\n",
    "    # --------------------------------------------------------------------------\n",
    "    logger.info(\"‚úÖ Model validation PASSED\")\n",
    "\n",
    "    # Extract version from URI: models:/name/version\n",
    "    try:\n",
    "        version = model_uri.split(\"/\")[-1]\n",
    "        client.set_registered_model_alias(\n",
    "            name=model_name,\n",
    "            alias=\"Challenger\",\n",
    "            version=version,\n",
    "        )\n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"validation_status\",\n",
    "            value=\"passed\",\n",
    "        )\n",
    "        logger.info(\"Attached 'Challenger' alias to model version %s\", version)\n",
    "    except Exception as exc:\n",
    "        logger.warning(\"Failed to attach Challenger alias: %s\", exc)\n",
    "\n",
    "    emit_ci_output(\"model_validation\", \"PASSED\")\n",
    "    set_task_value(\"model_validation\", \"PASSED\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Emit promotion metadata\n",
    "    # --------------------------------------------------------------------------\n",
    "    emit_ci_output(\"candidate_run_id\", candidate_run_id)\n",
    "    set_task_value(\"candidate_run_id\", candidate_run_id)\n",
    "\n",
    "    emit_ci_output(\"model_uri\", model_uri)\n",
    "    set_task_value(\"model_uri\", model_uri)\n",
    "\n",
    "    logger.info(\"Model validation completed successfully.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CLI Entrypoint\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(description=\"Honeywell MLOps Model Validation\")\n",
    "    \n",
    "    # parser.add_argument(\"--env\", default=\"honeywell_mlops_dev\")\n",
    "    # parser.add_argument(\"--model_name\", default=\"honeywell_mlops_dev.honeywell.ev_battery_charging_model_basic\")\n",
    "    # parser.add_argument(\"--model_uri\", default=\"models:/honeywell_mlops_dev.honeywell.ev_battery_charging_model_basic/4\")\n",
    "    # parser.add_argument(\"--candidate_run_id\", default=\"090e59afc8d3467e9c1c28a6916e9ec3\")\n",
    "\n",
    "    # real test case\n",
    "\n",
    "    # parser.add_argument(\"--env\", required=True, help=\"Environment name\")\n",
    "    # parser.add_argument(\"--model_name\", required=True, help=\"Registered model name\")\n",
    "    # parser.add_argument(\n",
    "    #     \"--model_uri\",\n",
    "    #     required=True,\n",
    "    #     help=\"Model URI in Unity Catalog (models:/catalog.schema.model/version)\",\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--candidate_run_id\",\n",
    "    #     required=True,\n",
    "    #     help=\"MLflow run_id of trained candidate model\",\n",
    "    # )\n",
    "\n",
    "    # args, unknown = parser.parse_known_args()\n",
    "    # if unknown:\n",
    "    #     logger.info(\"Ignoring unknown args: %s\", unknown)\n",
    "\n",
    "\n",
    "    def get_widget(name: str, default: str = \"\") -> str:\n",
    "        try:\n",
    "            return dbutils.widgets.get(name)\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Read widgets\n",
    "    # ------------------------------------------------------------------\n",
    "    arg_env = get_widget(\"env\", \"dev\")\n",
    "    arg_model_name = get_widget(\"model_name\")\n",
    "    arg_latest_version = get_widget(\"latest_version\")\n",
    "    arg_candidate_run_id = get_widget(\"candidate_run_id\")\n",
    "\n",
    "    if not all([arg_model_name, arg_latest_version, arg_candidate_run_id]):\n",
    "        raise RuntimeError(\"model_name, latest_version, candidate_run_id are required\")\n",
    "        # In notebooks, you can use dbutils.notebook.exit() or sys.exit(1)\n",
    "        # sys.exit(1)\n",
    "\n",
    "    model_uri = f\"models:/{arg_model_name}/{arg_latest_version}\"\n",
    "    logger.info(\"Using model_uri: %s\", model_uri)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Install model dependencies\n",
    "    # ------------------------------------------------------------------\n",
    "    deps_path = mlflow.pyfunc.get_model_dependencies(model_uri)\n",
    "    logger.info(\"üì¶ Model deps file: %s\", deps_path)\n",
    "\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", deps_path])\n",
    "    # ------------------------------------------------------------------\n",
    "    # Force Python to use pip-installed packages first\n",
    "    # ------------------------------------------------------------------\n",
    "    venv_site = site.getsitepackages()[0]\n",
    "    if venv_site not in sys.path:\n",
    "        sys.path.insert(0, venv_site)\n",
    "\n",
    "    logger.info(\"üîß Using venv site-packages: %s\", venv_site)\n",
    "    logger.info(\"üîß sys.path[0] = %s\", sys.path[0])\n",
    "\n",
    "    logger.info(\"‚úÖ Model dependencies installed & activated\")\n",
    "    try:\n",
    "        validate_candidate_model(\n",
    "            env=arg_env,\n",
    "            model_name=f\"{arg_model_name}\",\n",
    "            model_uri=f\"models:/{arg_model_name}/{arg_latest_version}\",\n",
    "            candidate_run_id=arg_candidate_run_id,\n",
    "        )\n",
    "    except Exception:\n",
    "        logger.exception(\"Model validation failed\")\n",
    "        sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
